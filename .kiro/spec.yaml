# Prompt Graveyard Evaluation Specification
# Spooky AI Dashboard for Multi-LLM Prompt Performance Analysis

metadata:
  name: "prompt-graveyard"
  version: "1.0.0"
  description: "Halloween-themed prompt evaluation system across multiple LLMs"
  author: "Kiroween Hackathon Team"
  created: "2024-11-05"
  
# LLM Provider Configurations
llm_providers:
  groq_llama3:
    name: "Groq LLaMA3"
    provider: "groq"
    model: "llama3-8b-8192"
    api_endpoint: "https://api.groq.com/openai/v1/chat/completions"
    api_key_env: "GROQ_API_KEY"
    max_tokens: 4096
    temperature: 0.7
    timeout: 30
    rate_limit:
      requests_per_minute: 30
      tokens_per_minute: 6000
    cost_per_token:
      input: 0.00000005  # $0.05 per 1M tokens
      output: 0.00000008 # $0.08 per 1M tokens
    zombie_threshold: 0.6
    
  openai_gpt35:
    name: "OpenAI GPT-3.5 Turbo"
    provider: "openai"
    model: "gpt-3.5-turbo"
    api_endpoint: "https://api.openai.com/v1/chat/completions"
    api_key_env: "OPENAI_API_KEY"
    max_tokens: 4096
    temperature: 0.7
    timeout: 30
    rate_limit:
      requests_per_minute: 60
      tokens_per_minute: 90000
    cost_per_token:
      input: 0.0000005   # $0.50 per 1M tokens
      output: 0.0000015  # $1.50 per 1M tokens
    zombie_threshold: 0.6
    
  openai_gpt4:
    name: "OpenAI GPT-4"
    provider: "openai"
    model: "gpt-4"
    api_endpoint: "https://api.openai.com/v1/chat/completions"
    api_key_env: "OPENAI_API_KEY"
    max_tokens: 4096
    temperature: 0.7
    timeout: 45
    rate_limit:
      requests_per_minute: 10
      tokens_per_minute: 10000
    cost_per_token:
      input: 0.00003     # $30 per 1M tokens
      output: 0.00006    # $60 per 1M tokens
    zombie_threshold: 0.65
    enabled: false  # Optional provider for premium evaluations

# Evaluation Metrics Configuration
evaluation_metrics:
  latency:
    name: "Response Latency"
    description: "Time taken for LLM to generate response"
    unit: "milliseconds"
    weight: 0.2
    scoring:
      excellent: { min: 0, max: 1000, score: 1.0 }      # < 1 second
      good: { min: 1000, max: 3000, score: 0.8 }        # 1-3 seconds
      acceptable: { min: 3000, max: 8000, score: 0.6 }  # 3-8 seconds
      poor: { min: 8000, max: 15000, score: 0.4 }       # 8-15 seconds
      zombie: { min: 15000, max: 999999, score: 0.2 }   # > 15 seconds
      
  cost_efficiency:
    name: "Cost per Response"
    description: "Total cost for generating the response"
    unit: "USD"
    weight: 0.15
    scoring:
      excellent: { min: 0, max: 0.001, score: 1.0 }     # < $0.001
      good: { min: 0.001, max: 0.005, score: 0.8 }      # $0.001-0.005
      acceptable: { min: 0.005, max: 0.02, score: 0.6 } # $0.005-0.02
      poor: { min: 0.02, max: 0.1, score: 0.4 }         # $0.02-0.1
      zombie: { min: 0.1, max: 999, score: 0.2 }        # > $0.1
      
  semantic_accuracy:
    name: "Semantic Accuracy"
    description: "How well the response addresses the prompt intent"
    unit: "score"
    weight: 0.35
    scoring_method: "llm_judge"  # Use another LLM to judge accuracy
    judge_prompt: |
      Rate the semantic accuracy of this response to the given prompt on a scale of 0.0 to 1.0.
      Consider: relevance, completeness, factual accuracy, and adherence to instructions.
      
      Prompt: {prompt}
      Response: {response}
      
      Provide only a numeric score between 0.0 and 1.0:
    scoring:
      excellent: { min: 0.9, max: 1.0, score: 1.0 }
      good: { min: 0.75, max: 0.9, score: 0.8 }
      acceptable: { min: 0.6, max: 0.75, score: 0.6 }
      poor: { min: 0.4, max: 0.6, score: 0.4 }
      zombie: { min: 0.0, max: 0.4, score: 0.2 }
      
  coherence:
    name: "Response Coherence"
    description: "Logical flow and readability of the response"
    unit: "score"
    weight: 0.2
    scoring_method: "automated"
    algorithms:
      - "sentence_similarity"
      - "topic_consistency"
      - "readability_score"
    scoring:
      excellent: { min: 0.85, max: 1.0, score: 1.0 }
      good: { min: 0.7, max: 0.85, score: 0.8 }
      acceptable: { min: 0.55, max: 0.7, score: 0.6 }
      poor: { min: 0.4, max: 0.55, score: 0.4 }
      zombie: { min: 0.0, max: 0.4, score: 0.2 }
      
  creativity:
    name: "Creative Quality"
    description: "Originality and creative value of the response"
    unit: "score"
    weight: 0.1
    scoring_method: "hybrid"  # Combination of automated and LLM judge
    enabled_for_prompts: ["creative_writing", "brainstorming", "storytelling"]
    scoring:
      excellent: { min: 0.8, max: 1.0, score: 1.0 }
      good: { min: 0.65, max: 0.8, score: 0.8 }
      acceptable: { min: 0.5, max: 0.65, score: 0.6 }
      poor: { min: 0.3, max: 0.5, score: 0.4 }
      zombie: { min: 0.0, max: 0.3, score: 0.2 }

# Zombie Classification Rules
zombie_classification:
  overall_threshold: 0.6  # Prompts scoring below this become zombies
  critical_metrics:       # Metrics that can trigger zombie status individually
    - metric: "semantic_accuracy"
      threshold: 0.4
    - metric: "coherence" 
      threshold: 0.4
  severity_levels:
    shambling_zombie:     # Mild performance issues
      score_range: { min: 0.5, max: 0.6 }
      visual_theme: "slightly_decayed"
      revival_priority: "low"
    rotting_zombie:       # Moderate performance issues
      score_range: { min: 0.3, max: 0.5 }
      visual_theme: "moderately_decayed"
      revival_priority: "medium"
    skeletal_zombie:      # Severe performance issues
      score_range: { min: 0.0, max: 0.3 }
      visual_theme: "heavily_decayed"
      revival_priority: "high"

# Data Storage Configuration
storage:
  primary:
    type: "sqlite"
    path: "./data/graveyard.db"
    backup_enabled: true
    backup_interval: "24h"
    
  results_export:
    type: "json"
    path: "./data/results.json"
    format: "jsonlines"  # One JSON object per line for streaming
    auto_export: true
    export_interval: "1h"
    
  vector_store:
    type: "chromadb"
    enabled: true
    path: "./data/chroma_db"
    collection_name: "prompt_embeddings"
    embedding_model: "sentence-transformers/all-MiniLM-L6-v2"
    similarity_threshold: 0.8
    use_cases:
      - "duplicate_detection"
      - "semantic_clustering" 
      - "revival_suggestion_similarity"

# Revival Agent Configuration
revival_agent:
  enabled: true
  trigger_conditions:
    - "zombie_classification"
    - "user_request"
    - "scheduled_batch"
  
  improvement_strategies:
    clarity_enhancement:
      description: "Improve prompt clarity and specificity"
      weight: 0.3
      techniques:
        - "add_context"
        - "specify_format"
        - "clarify_intent"
        
    instruction_optimization:
      description: "Optimize instruction structure and wording"
      weight: 0.25
      techniques:
        - "step_by_step_breakdown"
        - "example_provision"
        - "constraint_specification"
        
    context_enrichment:
      description: "Add relevant context and background"
      weight: 0.25
      techniques:
        - "domain_context"
        - "audience_specification"
        - "use_case_examples"
        
    format_standardization:
      description: "Standardize output format requirements"
      weight: 0.2
      techniques:
        - "output_template"
        - "structure_specification"
        - "length_guidelines"
  
  suggestion_limit: 3
  auto_test_suggestions: true
  success_threshold: 0.75  # Suggestions must achieve this score to be considered successful

# Evaluation Pipeline Configuration
evaluation_pipeline:
  batch_size: 5
  parallel_evaluations: 3
  retry_attempts: 2
  retry_delay: "exponential"  # 1s, 2s, 4s, etc.
  
  evaluation_stages:
    - name: "preprocessing"
      steps: ["prompt_validation", "duplicate_check", "sanitization"]
      
    - name: "llm_evaluation"
      steps: ["parallel_llm_calls", "response_collection", "error_handling"]
      
    - name: "metrics_calculation"
      steps: ["latency_measurement", "cost_calculation", "quality_assessment"]
      
    - name: "classification"
      steps: ["zombie_detection", "severity_assessment", "revival_triggering"]
      
    - name: "storage"
      steps: ["database_insert", "vector_embedding", "results_export"]

# Monitoring and Alerting
monitoring:
  enabled: true
  metrics_collection_interval: "5m"
  
  alerts:
    high_zombie_rate:
      condition: "zombie_rate > 0.7 over 1h"
      severity: "warning"
      notification: "webhook"
      
    llm_provider_failure:
      condition: "provider_error_rate > 0.5 over 10m"
      severity: "critical"
      notification: "webhook"
      
    cost_threshold_exceeded:
      condition: "hourly_cost > 10.0"
      severity: "warning"
      notification: "webhook"

# API Configuration
api:
  rate_limiting:
    requests_per_minute: 100
    burst_limit: 20
    
  authentication:
    enabled: false  # For hackathon demo
    # type: "api_key"  # Enable for production
    
  cors:
    enabled: true
    origins: ["http://localhost:3000", "http://localhost:5173"]
    
  webhooks:
    zombie_detected:
      url: "${WEBHOOK_URL}/zombie-alert"
      enabled: false
      
    revival_success:
      url: "${WEBHOOK_URL}/revival-celebration"
      enabled: false

# Development and Testing
development:
  mock_llm_responses: false  # Set to true for testing without API calls
  sample_prompts_path: "./data/sample_prompts.json"
  test_data_generation: true
  
  performance_testing:
    enabled: true
    concurrent_users: 10
    test_duration: "5m"
    target_rps: 5