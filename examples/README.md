# üéÉ Prompt Graveyard Examples üéÉ

This directory contains example prompts and demonstration scripts to showcase the Prompt Graveyard evaluation system.

## üìÅ Contents

### Sample Prompts (`sample-prompts/`)

Four example prompts demonstrating different quality levels:

1. **`good-prompt.txt`** ‚ú®
   - Well-structured, detailed prompt with clear requirements
   - Expected to score high and remain "alive"
   - Demonstrates best practices for prompt engineering

2. **`zombie-prompt.txt`** üíÄ
   - Extremely vague, minimal prompt
   - Expected to become a "skeletal zombie" (severe issues)
   - Shows what happens with poor prompt quality

3. **`creative-prompt.txt`** üé®
   - Creative storytelling prompt with good structure
   - Tests creativity metrics
   - Should score well overall

4. **`mediocre-prompt.txt`** üòê
   - Basic prompt with some structure but lacking detail
   - Expected to be borderline (shambling zombie or barely alive)
   - Shows the middle ground

## üöÄ Running the Demo

### Quick Start

```bash
# Run the evaluation demo
python examples/run-evaluation-demo.py
```

### What the Demo Does

The demo script will:

1. **Load Sample Prompts** - Reads all `.txt` files from `sample-prompts/`
2. **Evaluate Each Prompt** - Simulates evaluation with mock LLM providers
3. **Calculate Metrics** - Computes latency, cost, semantic accuracy, etc.
4. **Classify Zombies** - Determines which prompts are "zombies"
5. **Generate Revival Suggestions** - Creates improvement suggestions for zombies
6. **Save Results** - Appends results to `data/results.json` in JSONL format
7. **Display Summary** - Shows statistics and zombie breakdown

### Expected Output

```
üéÉüëªüéÉüëªüéÉüëªüéÉüëªüéÉüëªüéÉüëªüéÉüëªüéÉüëªüéÉüëªüéÉüëªüéÉüëªüéÉ

     üèöÔ∏è  PROMPT GRAVEYARD EVALUATION DEMO  üèöÔ∏è
     
     Demonstrating the complete evaluation pipeline
     with sample prompts of varying quality
     
üéÉüëªüéÉüëªüéÉüëªüéÉüëªüéÉüëªüéÉüëªüéÉüëªüéÉüëªüéÉüëªüéÉüëªüéÉüëªüéÉ

üîÆ Initialized 2 LLM providers
üìÅ Found 4 sample prompts

============================================================
üéÉ Evaluating: creative-prompt.txt
============================================================
üìù Prompt: Write a spooky Halloween story about an AI...
üìè Length: 42 words

üîÆ Calling LLM providers...
  ‚ö° OpenAI GPT-3.5... ‚úì (2400ms, $0.0156)
  ‚ö° Groq LLaMA3... ‚úì (1440ms, $0.0078)

üìä Calculating metrics...
  üìà Semantic Accuracy: 0.85 (good)
  ‚ö° Avg Latency: 1920ms (good)
  üí∞ Total Cost: $0.0234 (acceptable)
  üéØ Overall Score: 0.78

üßü‚Äç‚ôÇÔ∏è Zombie Classification:
  Status: ALIVE ‚ú®
  Severity: alive
  Priority: none

[... continues for each prompt ...]

============================================================
üìä EVALUATION SUMMARY
============================================================
Total Prompts Evaluated: 4
Living Prompts: 2 ‚ú®
Zombie Prompts: 2 üßü‚Äç‚ôÇÔ∏è
Zombie Rate: 50.0%
Average Score: 0.62
Total Cost: $0.0456

üé≠ Zombie Breakdown:
  skeletal_zombie: 1
  shambling_zombie: 1

============================================================
üéâ Evaluation Demo Complete!
============================================================
```

## üîó Integration with API

After running the demo, the results are saved to `data/results.json`. You can then:

### 1. Start the API Server

```bash
cd server
npm run dev
```

### 2. Query the Results

```bash
# Get all results
curl "http://localhost:3001/api/results"

# Get zombie prompts
curl "http://localhost:3001/api/prompts/zombies"

# Get graveyard statistics
curl "http://localhost:3001/api/prompts/stats"

# Get aggregated metrics
curl "http://localhost:3001/api/results/metrics"
```

### 3. Test Revival System

```bash
# Get a zombie prompt ID from the results
ZOMBIE_ID=$(curl -s "http://localhost:3001/api/prompts/zombies?limit=1" | jq -r '.data.zombies[0].prompt_id')

# Get revival suggestions
curl "http://localhost:3001/api/revive/suggestions/$ZOMBIE_ID"

# Attempt to revive the zombie
curl -X POST "http://localhost:3001/api/revive" \
  -H "Content-Type: application/json" \
  -d "{\"prompt_id\": \"$ZOMBIE_ID\", \"suggestion_index\": 0}"
```

## üìù Creating Your Own Sample Prompts

Add new prompts to the `sample-prompts/` directory:

```bash
# Create a new prompt file
echo "Your prompt text here" > examples/sample-prompts/my-prompt.txt

# Run the evaluation
python examples/run-evaluation-demo.py
```

### Tips for Creating Sample Prompts

**For Good Prompts (High Scores):**
- Be specific and detailed
- Include clear requirements
- Specify desired format
- Provide context
- Use structured formatting

**For Zombie Prompts (Low Scores):**
- Be extremely vague
- Use minimal words
- Lack context or structure
- Have unclear intent
- Omit important details

## üé≠ Understanding the Results

### Zombie Classification

- **Alive** (Score ‚â• 0.6): Prompt performs well ‚ú®
- **Shambling Zombie** (0.5-0.6): Mild issues, low priority üßü
- **Rotting Zombie** (0.3-0.5): Moderate issues, medium priority üßü‚Äç‚ôÇÔ∏è
- **Skeletal Zombie** (< 0.3): Severe issues, high priority üíÄ

### Metrics Explained

- **Semantic Accuracy**: How well responses address the prompt
- **Latency**: Average response time across LLM providers
- **Cost Efficiency**: Total cost of generating responses
- **Overall Score**: Weighted combination of all metrics

### Revival Suggestions

For zombie prompts, the system generates 3 improvement strategies:

1. **Clarity Enhancement**: Add context and structure
2. **Instruction Optimization**: Specify format and requirements
3. **Context Enrichment**: Add examples and use cases

## üß™ Testing the Complete Pipeline

### Full End-to-End Test

```bash
# 1. Run evaluation demo
python examples/run-evaluation-demo.py

# 2. Start API server (in another terminal)
cd server && npm run dev

# 3. Test API endpoints (in another terminal)
node server/test-api.js

# 4. View results in browser
open http://localhost:3001/api/
```

## üéÉ Next Steps

1. **Customize Prompts**: Modify sample prompts to test different scenarios
2. **Add More Samples**: Create additional prompts in various domains
3. **Test Real LLMs**: Replace mock providers with actual API calls
4. **Build Frontend**: Create a React dashboard to visualize results
5. **Automate**: Set up agent hooks to evaluate prompts automatically

---

*May your prompts be ever living! üåü*